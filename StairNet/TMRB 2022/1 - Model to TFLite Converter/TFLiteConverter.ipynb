{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFLiteConverter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMinxSNyknw1",
        "outputId": "614f25fb-88b8-45b6-d712-e858f1ca693d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "wZq7Khi7lI32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j20e83gelFFq",
        "outputId": "5a56690b-d5b7-4de5-de9d-8ea79d0dbfbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Model"
      ],
      "metadata": {
        "id": "ysybQrCwnpI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_location = \"/content/drive/MyDrive/CV_Research/StairNet_Results/Project1_Final_Model.h5\""
      ],
      "metadata": {
        "id": "jaeyIrrGnqWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create  Generator to generate data for Quantization (To do later for use as calibration data)"
      ],
      "metadata": {
        "id": "Te5c2vnG0Gjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ],
      "metadata": {
        "id": "dS9-bCkQ0NOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# project_id = 'stairnet'\n",
        "# !gcloud config set project {project_id}"
      ],
      "metadata": {
        "id": "0o9G3a6h0WGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test to see if dataset location is correct\n",
        "# ! gsutil ls -al gs://stairnet_bucket"
      ],
      "metadata": {
        "id": "AtaaLHXQ0XSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCS_DS_PATH = 'gs://stairnet_bucket'"
      ],
      "metadata": {
        "id": "UCvpKBbu0YvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCS_PATH_SELECT = { # available image sizes\n",
        "#     256: GCS_DS_PATH + '/Split_StairNet_TFRecord',\n",
        "# }\n",
        "# GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
        "# print(GCS_PATH)\n",
        "\n",
        "\n",
        "# TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n",
        "# VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n",
        "# TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') # predictions on this dataset should be submitted for the competition\n",
        "\n",
        "# CLASSES = ['IS', 'IS-LG', 'LG', 'LG-IS']  "
      ],
      "metadata": {
        "id": "-UnSmVlt0bvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def decode_image(image_data):\n",
        "#     image = tf.io.decode_raw(image_data, tf.uint8)\n",
        "#     # image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "#     # image = image_data\n",
        "#     image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "#     image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
        "#     image = tf.image.random_crop(value=image, size=(224, 224, 3))\n",
        "#     return image\n",
        "\n",
        "# def read_labeled_tfrecord(example):\n",
        "#     LABELED_TFREC_FORMAT = {\n",
        "#         \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "#         \"label\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
        "#     }\n",
        "#     example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "#     image = decode_image(example['image'])\n",
        "#     label = tf.cast(example['label'], tf.int32)\n",
        "#     return image, label # returns a dataset of (image, label) pairs\n",
        "\n",
        "# def read_unlabeled_tfrecord(example):\n",
        "#     UNLABELED_TFREC_FORMAT = {\n",
        "#         \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "#         \"label\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
        "#     }\n",
        "#     example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
        "#     image = decode_image(example['image'])\n",
        "#     idnum = example['label']\n",
        "#     return image, idnum # returns a dataset of image(s)\n",
        "\n",
        "# def load_dataset(filenames, labeled=True, ordered=False):\n",
        "#     # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "#     # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "#     ignore_order = tf.data.Options()\n",
        "#     if not ordered:\n",
        "#         ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "\n",
        "#     dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "#     dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "#     dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
        "#     # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
        "#     return dataset\n",
        "\n",
        "# def data_augment(image, label):\n",
        "#     # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
        "#     # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
        "#     # of the TPU while the TPU itself is computing gradients.\n",
        "#     image = tf.image.random_flip_left_right(image)\n",
        "#     #image = tf.image.random_saturation(image, 0, 2)\n",
        "#     return image, label   \n",
        "\n",
        "# def get_training_dataset():\n",
        "#     dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
        "#     dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "#     dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
        "#     # for hyperparameter testing using random seed to shuffle the data the same to elimite this variable from results\n",
        "#     dataset = dataset.shuffle(count_data_items(TRAINING_FILENAMES)/10, seed=SEED_NUMBER, reshuffle_each_iteration=None)\n",
        "#     dataset = dataset.batch(BATCH_SIZE)\n",
        "#     dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "#     return dataset\n",
        "\n",
        "# def get_validation_dataset(ordered=False):\n",
        "#     dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
        "#     dataset = dataset.batch(BATCH_SIZE)\n",
        "#     # for hyperparameter testing using random seed to shuffle the data the same to elimite this variable from results\n",
        "#     dataset = dataset.shuffle(count_data_items(VALIDATION_FILENAMES)/10, seed=SEED_NUMBER, reshuffle_each_iteration=None)\n",
        "#     dataset = dataset.cache()\n",
        "#     dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "#     return dataset\n",
        "\n",
        "# def get_test_dataset(ordered=False):\n",
        "#     dataset = load_dataset(TEST_FILENAMES, labeled=True, ordered=ordered)\n",
        "#     dataset = dataset.batch(BATCH_SIZE)\n",
        "#     # for hyperparameter testing using random seed to shuffle the data the same to elimite this variable from results\n",
        "#     dataset = dataset.shuffle(count_data_items(TEST_FILENAMES)/10, seed=SEED_NUMBER, reshuffle_each_iteration=None)\n",
        "#     dataset = dataset.cache()\n",
        "#     dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "#     return dataset \n",
        "\n",
        "# def count_data_items(filenames):\n",
        "#     n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "#     return np.sum(n)\n",
        "\n",
        "# NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "# NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
        "# NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
        "# STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "# VALIDATION_STEPS = -(-NUM_VALIDATION_IMAGES // BATCH_SIZE) # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "# TEST_STEPS = -(-NUM_TEST_IMAGES // BATCH_SIZE)             # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "# print('Dataset: {} training images, {} validation images, {} test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
      ],
      "metadata": {
        "id": "yuyzLDGn0p76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generator():\n",
        "#   data = get_validation_dataset()\n",
        "#   data = data.unbatch().batch(20)\n",
        "#   for _ in range(num_calibration_steps):  \n",
        "#     image, = data.take(1)\n",
        "#     yield [image]"
      ],
      "metadata": {
        "id": "Lq46ipgX00r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert TF model to TF Lite"
      ],
      "metadata": {
        "id": "UKKBpZzIkxD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converter = tf.lite.TFLiteConverter.from_keras_model_file( '/content/drive/MyDrive/CV_Research/StairNet_Results/Project1_Final_Model.h5' ) # Your model's name\n",
        "# model = converter.convert()\n",
        "# file = open( 'model.tflite' , 'wb' ) \n",
        "# file.write( model )\n",
        "\n",
        "# WHOLE MODEL\n",
        "tflite_model = tf.keras.models.load_model('/content/drive/MyDrive/CV_Research/StairNet_Results/StairNet_Model_ICORR_Final.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(tflite_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_save = converter.convert()\n",
        "open(\"generated.tflite\", \"wb\").write(tflite_save)"
      ],
      "metadata": {
        "id": "tNot2Ku7kwao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616382d9-b0fc-4b1a-a53a-796977259b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpe2wplgnk/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpe2wplgnk/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4485984"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}